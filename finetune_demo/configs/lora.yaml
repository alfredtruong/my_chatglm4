data_config:
  train_file: train.jsonl  # File path of training dataset.
  val_file: dev.jsonl  # File path of validation dataset.
  test_file: dev.jsonl  # File path of test dataset.
  num_proc: 1  # Number of processes to use when loading data.

combine: True  # Whether to combine train and val data for training.
max_input_length: 512  # Maximum length of input sequence.
max_output_length: 512  # Maximum length of output sequence.

training_args:
  # see `transformers.Seq2SeqTrainingArguments`
  output_dir: ./output  # Directory for saving model and other outputs.
  max_steps: 3000  # Maximum number of training steps.
  # needed to be fit for the dataset
  learning_rate: 5e-4  # Learning rate for the optimizer.
  # settings for data loading
  per_device_train_batch_size: 1  # Training batch size per device (such as GPU).
  dataloader_num_workers: 16  # Number of worker threads to use when loading data.
  remove_unused_columns: false  # Whether to remove unused columns in data.
  # settings for saving checkpoints
  save_strategy: steps  # Model saving strategy (for example, how many steps to save).
  save_steps: 500  # How many steps to save the model.
  # settings for logging
  log_level: info  # Log level (such as info).
  logging_strategy: steps  # Logging strategy.
  logging_steps: 10  # How many steps to log at.
  # settings for evaluation
  per_device_eval_batch_size: 4  # Per-device evaluation batch size.
  eval_strategy: steps  # Evaluation strategy (e.g. how many steps to evaluate at).
  eval_steps: 500  # How many steps to evaluate at.
  # settings for optimizer
  # adam_epsilon: 1e-6
  # uncomment the following line to detect nan or inf values
  # debug: underflow_overflow
  predict_with_generate: true  # Whether to use generation mode for prediction.
  # see `transformers.GenerationConfig`
  generation_config:
    max_new_tokens: 512  # Maximum number of new tokens to generate.
  # set your absolute deepspeed path here
  # deepspeed: configs/ds_zero_3.json
peft_config:
  peft_type: LORA  # Type of parameter tuning to use (supports LORA and PREFIX_TUNING).
  task_type: CAUSAL_LM  # Task type, here is causal language model (don't change).
  r: 8  # Rank of LoRA.
  lora_alpha: 32  # Scaling factor of LoRA.
  lora_dropout: 0.1  # Dropout probability to use in LoRA layer.
  target_modules: ["query_key_value"]  # Target modules for parameter tuning.